# LangChainì„ í™œìš©í•œ IR ê²½ì§„ëŒ€íšŒ Submission ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

LangChainì„ ì‚¬ìš©í•˜ì—¬ vector storeë¥¼ êµ¬ì¶•í•˜ê³  ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ submission.csvë¥¼ ìƒì„±í•˜ëŠ” ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
2. [ë°ì´í„° ì¤€ë¹„ ë° Vector Store êµ¬ì¶•](#ë°ì´í„°-ì¤€ë¹„-ë°-vector-store-êµ¬ì¶•)
3. [ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„](#ê²€ìƒ‰-ì‹œìŠ¤í…œ-êµ¬í˜„)
4. [Submission íŒŒì¼ ìƒì„±](#submission-íŒŒì¼-ìƒì„±)
5. [ê³ ê¸‰ ìµœì í™” ê¸°ë²•](#ê³ ê¸‰-ìµœì í™”-ê¸°ë²•)

---

## ğŸ› ï¸ í™˜ê²½ ì„¤ì •

### í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install langchain langchain-openai langchain-community
pip install chromadb faiss-cpu  # vector store ì˜µì…˜
pip install pandas numpy
```

### ê¸°ë³¸ import ë° ì„¤ì •

```python
import pandas as pd
import json
import os
from typing import List, Dict, Optional

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma, FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "your-api-key"
```

---

## ğŸ“š ë°ì´í„° ì¤€ë¹„ ë° Vector Store êµ¬ì¶•

### 1. ë¬¸ì„œ ë°ì´í„° ë¡œë”©

```python
class DocumentLoader:
    def __init__(self, data_path: str):
        self.data_path = data_path

    def load_documents(self) -> List[Document]:
        """ë¬¸ì„œ ë°ì´í„°ë¥¼ LangChain Document í˜•íƒœë¡œ ë¡œë”©"""
        # CSV ë˜ëŠ” JSON í˜•íƒœì˜ ë¬¸ì„œ ë°ì´í„° ë¡œë”©
        df = pd.read_csv(self.data_path)

        documents = []
        for _, row in df.iterrows():
            doc = Document(
                page_content=row['content'],
                metadata={
                    'doc_id': row['doc_id'],
                    'source': row['source']
                }
            )
            documents.append(doc)

        print(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ")
        return documents


# ì‚¬ìš© ì˜ˆì‹œ
loader = DocumentLoader("documents.csv")
documents = loader.load_documents()
```

### 2. ë¬¸ì„œ ë¶„í•  (Chunking)

```python
class DocumentChunker:
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
        chunked_docs = []

        for doc in documents:
            chunks = self.text_splitter.split_text(doc.page_content)

            for i, chunk in enumerate(chunks):
                chunked_doc = Document(
                    page_content=chunk,
                    metadata={
                        **doc.metadata,
                        'chunk_id': i,
                        'total_chunks': len(chunks)
                    }
                )
                chunked_docs.append(chunked_doc)

        print(f"ì´ {len(chunked_docs)}ê°œ ì²­í¬ ìƒì„±")
        return chunked_docs


# ì‚¬ìš© ì˜ˆì‹œ
chunker = DocumentChunker(chunk_size=500, chunk_overlap=50)
chunked_documents = chunker.chunk_documents(documents)
```

### 3. Vector Store êµ¬ì¶•

```python
class VectorStoreBuilder:
    def __init__(self, embedding_model: str = "text-embedding-3-small"):
        self.embeddings = OpenAIEmbeddings(model=embedding_model)

    def build_chroma_store(self, documents: List[Document],
                           persist_directory: str = "./chroma_db") -> Chroma:
        """ChromaDBë¥¼ ì‚¬ìš©í•œ Vector Store êµ¬ì¶•"""
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=persist_directory,
            collection_name="science_docs"
        )

        print(f"ChromaDB Vector Store êµ¬ì¶• ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        return vectorstore

    def build_faiss_store(self, documents: List[Document]) -> FAISS:
        """FAISSë¥¼ ì‚¬ìš©í•œ Vector Store êµ¬ì¶•"""
        vectorstore = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )

        print(f"FAISS Vector Store êµ¬ì¶• ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        return vectorstore

    def load_existing_store(self, persist_directory: str) -> Chroma:
        """ê¸°ì¡´ Vector Store ë¡œë”©"""
        vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings,
            collection_name="science_docs"
        )
        return vectorstore


# ì‚¬ìš© ì˜ˆì‹œ
builder = VectorStoreBuilder()

# ìƒˆë¡œ êµ¬ì¶•
vectorstore = builder.build_chroma_store(chunked_documents)

# ë˜ëŠ” ê¸°ì¡´ ê²ƒ ë¡œë”©
# vectorstore = builder.load_existing_store("./chroma_db")
```

---

## ğŸ” ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„

### 1. ì§ˆì˜ ë¶„ì„ ì‹œìŠ¤í…œ

```python
class QueryAnalyzer:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        # ì§ˆì˜ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ íŒë‹¨í•´ì£¼ì„¸ìš”:
1. ê³¼í•™ ìƒì‹ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì—¬ë¶€
2. ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€ ì—¬ë¶€

ê³¼í•™ ìƒì‹ ì§ˆë¬¸ì˜ ì˜ˆ: "ê´‘í•©ì„±ì€ ì–´ë–»ê²Œ ì¼ì–´ë‚˜ë‚˜ìš”?", "ì¤‘ë ¥ì˜ ì›ë¦¬ê°€ ë­ì•¼?"
ì¼ìƒ ëŒ€í™”ì˜ ì˜ˆ: "ê³ ë§ˆì›Œ", "ì˜ ì§€ë‚´?", "ë„¤ê°€ ëŒ€ë‹µì„ ì˜í•´ì¤˜ì„œ ì‹ ë‚˜"

ì‘ë‹µ í˜•ì‹:
{{
    "is_science_query": true/false,
    "needs_search": true/false,
    "reasoning": "íŒë‹¨ ê·¼ê±°"
}}"""),
            ("user", "{query}")
        ])

        # Standalone ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.standalone_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì£¼ì–´ì§„ ëŒ€í™” ë§¥ë½ê³¼ í›„ì† ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ë¬¸ì„œ ê²€ìƒ‰ì— ì í•©í•œ ë…ë¦½ì ì¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ìƒì„± ê·œì¹™:
1. ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
2. ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œ í¬í•¨
3. í•œêµ­ì–´ë¡œ ì‘ì„±
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ

ì˜ˆì‹œ:
ì…ë ¥: "ê·¸ê²ƒì˜ ì›ë¦¬ê°€ ë­ì•¼?" (ì•ì„  ëŒ€í™”ì—ì„œ "ê´‘í•©ì„±"ì— ëŒ€í•´ ë…¼ì˜)
ì¶œë ¥: "ê´‘í•©ì„±ì˜ ì›ë¦¬ì™€ ê³¼ì •"
"""),
            ("user", "ëŒ€í™” ë§¥ë½: {chat_history}\ní˜„ì¬ ì§ˆë¬¸: {question}")
        ])

    def analyze_query(self, query: str) -> Dict:
        """ì§ˆì˜ ë¶„ì„ ìˆ˜í–‰"""
        chain = self.analysis_prompt | self.llm

        try:
            response = chain.invoke({"query": query})
            result = json.loads(response.content)
            return result
        except:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            return {
                "is_science_query": True,
                "needs_search": True,
                "reasoning": "ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì ìš©"
            }

    def generate_standalone_query(self, question: str,
                                  chat_history: str = "") -> str:
        """ê²€ìƒ‰ìš© ë…ë¦½ ì¿¼ë¦¬ ìƒì„±"""
        chain = self.standalone_prompt | self.llm

        response = chain.invoke({
            "question": question,
            "chat_history": chat_history
        })

        return response.content.strip()


# ì‚¬ìš© ì˜ˆì‹œ
analyzer = QueryAnalyzer()

query = "ê¸°ì–µìƒì‹¤ì¦ì˜ ì›ì¸ì´ ë­ì•¼?"
analysis = analyzer.analyze_query(query)
print(f"ë¶„ì„ ê²°ê³¼: {analysis}")

if analysis["needs_search"]:
    standalone_query = analyzer.generate_standalone_query(query)
    print(f"ê²€ìƒ‰ ì¿¼ë¦¬: {standalone_query}")
```

### 2. ê²€ìƒ‰ ì‹œìŠ¤í…œ

```python
class IRSearchSystem:
    def __init__(self, vectorstore, top_k: int = 10):
        self.vectorstore = vectorstore
        self.retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": top_k}
        )
        self.analyzer = QueryAnalyzer()
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    def search(self, query: str, chat_history: str = "") -> List[str]:
        """ì£¼ì–´ì§„ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰"""

        # 1. ì§ˆì˜ ë¶„ì„
        analysis = self.analyzer.analyze_query(query)

        if not analysis["needs_search"]:
            return []  # ê²€ìƒ‰ ë¶ˆí•„ìš”í•œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        # 2. Standalone ì¿¼ë¦¬ ìƒì„±
        standalone_query = self.analyzer.generate_standalone_query(
            query, chat_history
        )

        # 3. ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = self.retriever.invoke(standalone_query)

        # 4. ë¬¸ì„œ ID ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
        doc_ids = []
        seen_ids = set()

        for doc in retrieved_docs:
            doc_id = doc.metadata.get('doc_id')
            if doc_id and doc_id not in seen_ids:
                doc_ids.append(doc_id)
                seen_ids.add(doc_id)

        return doc_ids[:3]  # Top-3 ë°˜í™˜

    def search_with_reranking(self, query: str,
                              chat_history: str = "") -> List[str]:
        """LLM ë¦¬ë­í‚¹ì„ í¬í•¨í•œ ê³ ê¸‰ ê²€ìƒ‰"""

        # 1. ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ë” ë§ì€ í›„ë³´ ì¶”ì¶œ
        analysis = self.analyzer.analyze_query(query)

        if not analysis["needs_search"]:
            return []

        standalone_query = self.analyzer.generate_standalone_query(
            query, chat_history
        )

        # ë” ë§ì€ í›„ë³´ ì¶”ì¶œ (10ê°œ)
        retrieved_docs = self.vectorstore.similarity_search(
            standalone_query, k=10
        )

        if not retrieved_docs:
            return []

        # 2. LLMì„ ì‚¬ìš©í•œ ë¦¬ë­í‚¹
        reranked_docs = self._rerank_with_llm(query, retrieved_docs)

        # 3. Top-3 ë¬¸ì„œ ID ë°˜í™˜
        doc_ids = []
        seen_ids = set()

        for doc in reranked_docs[:3]:
            doc_id = doc.metadata.get('doc_id')
            if doc_id and doc_id not in seen_ids:
                doc_ids.append(doc_id)
                seen_ids.add(doc_id)

        return doc_ids

    def _rerank_with_llm(self, query: str,
                         documents: List[Document]) -> List[Document]:
        """LLMì„ ì‚¬ìš©í•œ ë¬¸ì„œ ë¦¬ë­í‚¹"""

        rerank_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ë¬¸ì„œë“¤ì„ ë³´ê³ , ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì— ê°€ì¥ ìœ ìš©í•œ ìˆœì„œë¡œ ë¬¸ì„œë¥¼ ì •ë ¬í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹: ë¬¸ì„œ ì¸ë±ìŠ¤ë¥¼ ê´€ë ¨ì„±ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´ (ì˜ˆ: [2, 0, 1, 4, 3])
"""),
            ("user", """ì§ˆë¬¸: {query}

ë¬¸ì„œë“¤:
{documents}

ê´€ë ¨ì„±ì´ ë†’ì€ ìˆœì„œë¡œ ë¬¸ì„œ ì¸ë±ìŠ¤ë¥¼ ì •ë ¬í•´ì£¼ì„¸ìš”:""")
        ])

        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„
        doc_texts = []
        for i, doc in enumerate(documents):
            content = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
            doc_texts.append(f"ë¬¸ì„œ {i}: {content}")

        doc_string = "\n\n".join(doc_texts)

        try:
            chain = rerank_prompt | self.llm
            response = chain.invoke({
                "query": query,
                "documents": doc_string
            })

            # ìˆœì„œ íŒŒì‹±
            order_str = response.content.strip()
            order = eval(order_str)  # ì£¼ì˜: ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë” ì•ˆì „í•œ íŒŒì‹± í•„ìš”

            return [documents[i] for i in order if i < len(documents)]

        except:
            # ë¦¬ë­í‚¹ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìˆœì„œ ë°˜í™˜
            return documents


# ì‚¬ìš© ì˜ˆì‹œ
search_system = IRSearchSystem(vectorstore, top_k=10)

# ê¸°ë³¸ ê²€ìƒ‰
query = "ê´‘í•©ì„±ì—ì„œ ì‚°ì†ŒëŠ” ì–´ë–»ê²Œ ìƒì„±ë˜ë‚˜ìš”?"
doc_ids = search_system.search(query)
print(f"ê²€ìƒ‰ ê²°ê³¼: {doc_ids}")

# ë¦¬ë­í‚¹ í¬í•¨ ê²€ìƒ‰
doc_ids_reranked = search_system.search_with_reranking(query)
print(f"ë¦¬ë­í‚¹ ê²°ê³¼: {doc_ids_reranked}")
```

---

## ğŸ“¤ Submission íŒŒì¼ ìƒì„±

### 1. í‰ê°€ ë°ì´í„° ì²˜ë¦¬

```python
class SubmissionGenerator:
    def __init__(self, search_system: IRSearchSystem):
        self.search_system = search_system

    def load_evaluation_data(self, eval_path: str) -> Dict:
        """í‰ê°€ ë°ì´í„° ë¡œë”©"""
        with open(eval_path, 'r', encoding='utf-8') as f:
            eval_data = json.load(f)

        print(f"í‰ê°€ ë°ì´í„° ë¡œë”©: {len(eval_data)}ê°œ ì¿¼ë¦¬")
        return eval_data

    def extract_final_query(self, messages: List[Dict]) -> str:
        """ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ìµœì¢… ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ"""
        user_messages = [msg for msg in messages if msg.get('role') == 'user']
        if user_messages:
            return user_messages[-1]['content']
        return ""

    def extract_chat_history(self, messages: List[Dict]) -> str:
        """ëŒ€í™” ì´ë ¥ ì¶”ì¶œ"""
        history = []
        for msg in messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            history.append(f"{role}: {content}")

        return "\n".join(history)

    def generate_submission(self, eval_data: Dict,
                            output_path: str = "submission.csv"):
        """Submission íŒŒì¼ ìƒì„±"""

        results = []

        for eval_id, data in eval_data.items():
            messages = data.get('messages', [])

            # ìµœì¢… ì¿¼ë¦¬ì™€ ëŒ€í™” ì´ë ¥ ì¶”ì¶œ
            final_query = self.extract_final_query(messages)
            chat_history = self.extract_chat_history(messages)

            if not final_query:
                doc_ids = []
            else:
                # ê²€ìƒ‰ ìˆ˜í–‰
                doc_ids = self.search_system.search_with_reranking(
                    final_query, chat_history
                )

            # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            doc_ids_str = json.dumps(doc_ids, ensure_ascii=False)

            results.append({
                'evaluation_id': eval_id,
                'doc_ids': doc_ids_str
            })

            print(f"ì²˜ë¦¬ ì™„ë£Œ: {eval_id} -> {len(doc_ids)}ê°œ ë¬¸ì„œ")

        # CSV íŒŒì¼ ì €ì¥
        df = pd.DataFrame(results)
        df.to_csv(output_path, index=False, encoding='utf-8')

        print(f"Submission íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_path}")
        return df


# ì‚¬ìš© ì˜ˆì‹œ
generator = SubmissionGenerator(search_system)

# í‰ê°€ ë°ì´í„° ë¡œë”©
eval_data = generator.load_evaluation_data("evaluation_data.json")

# Submission íŒŒì¼ ìƒì„±
submission_df = generator.generate_submission(eval_data)

print(submission_df.head())
```

### 2. ê²€ì¦ ë° ë””ë²„ê¹…

```python
class SubmissionValidator:
    def __init__(self):
        pass

    def validate_submission(self, submission_path: str) -> bool:
        """Submission íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            df = pd.read_csv(submission_path)

            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_cols = ['evaluation_id', 'doc_ids']
            if not all(col in df.columns for col in required_cols):
                print("âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½")
                return False

            # ê° í–‰ ê²€ì¦
            for idx, row in df.iterrows():
                eval_id = row['evaluation_id']
                doc_ids_str = row['doc_ids']

                try:
                    # JSON íŒŒì‹± í…ŒìŠ¤íŠ¸
                    doc_ids = json.loads(doc_ids_str)

                    # íƒ€ì… ê²€ì¦
                    if not isinstance(doc_ids, list):
                        print(f"âŒ {eval_id}: doc_idsê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")
                        return False

                    # ê¸¸ì´ ê²€ì¦
                    if len(doc_ids) > 3:
                        print(f"âŒ {eval_id}: ë¬¸ì„œ ê°œìˆ˜ê°€ 3ê°œ ì´ˆê³¼ ({len(doc_ids)}ê°œ)")
                        return False

                    # ë¬¸ì„œ ID í˜•ì‹ ê²€ì¦
                    for doc_id in doc_ids:
                        if not isinstance(doc_id, str) or len(doc_id) == 0:
                            print(f"âŒ {eval_id}: ì˜ëª»ëœ ë¬¸ì„œ ID í˜•ì‹")
                            return False

                except json.JSONDecodeError:
                    print(f"âŒ {eval_id}: JSON íŒŒì‹± ì˜¤ë¥˜")
                    return False

            print("âœ… Submission íŒŒì¼ ê²€ì¦ í†µê³¼")
            return True

        except Exception as e:
            print(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def analyze_submission(self, submission_path: str):
        """Submission í†µê³„ ë¶„ì„"""
        df = pd.read_csv(submission_path)

        doc_counts = []
        empty_count = 0

        for _, row in df.iterrows():
            doc_ids = json.loads(row['doc_ids'])
            doc_counts.append(len(doc_ids))
            if len(doc_ids) == 0:
                empty_count += 1

        print("=== Submission ë¶„ì„ ê²°ê³¼ ===")
        print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(df)}")
        print(f"ë¹ˆ ê²°ê³¼ ìˆ˜: {empty_count} ({empty_count / len(df) * 100:.1f}%)")
        print(f"í‰ê·  ë¬¸ì„œ ìˆ˜: {sum(doc_counts) / len(doc_counts):.2f}")
        print(f"ë¬¸ì„œ ìˆ˜ ë¶„í¬:")
        for i in range(4):
            count = doc_counts.count(i)
            print(f"  {i}ê°œ: {count}ê°œ ({count / len(doc_counts) * 100:.1f}%)")


# ì‚¬ìš© ì˜ˆì‹œ
validator = SubmissionValidator()

# ê²€ì¦
is_valid = validator.validate_submission("submission.csv")

if is_valid:
    # ë¶„ì„
    validator.analyze_submission("submission.csv")
```

---

## ğŸš€ ê³ ê¸‰ ìµœì í™” ê¸°ë²•

### 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

```python
class HybridSearchSystem(IRSearchSystem):
    def __init__(self, vectorstore, sparse_retriever=None):
        super().__init__(vectorstore)
        self.sparse_retriever = sparse_retriever  # BM25 ë“±

    def hybrid_search(self, query: str, chat_history: str = "") -> List[str]:
        """Dense + Sparse í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""

        analysis = self.analyzer.analyze_query(query)
        if not analysis["needs_search"]:
            return []

        standalone_query = self.analyzer.generate_standalone_query(
            query, chat_history
        )

        # Dense ê²€ìƒ‰ (Vector Store)
        dense_docs = self.vectorstore.similarity_search(standalone_query, k=8)

        # Sparse ê²€ìƒ‰ (BM25 ë“±)
        if self.sparse_retriever:
            sparse_docs = self.sparse_retriever.get_relevant_documents(
                standalone_query
            )[:8]
        else:
            sparse_docs = []

        # ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        combined_docs = dense_docs + sparse_docs
        seen_ids = set()
        unique_docs = []

        for doc in combined_docs:
            doc_id = doc.metadata.get('doc_id')
            if doc_id and doc_id not in seen_ids:
                unique_docs.append(doc)
                seen_ids.add(doc_id)

        # LLM ë¦¬ë­í‚¹
        reranked_docs = self._rerank_with_llm(query, unique_docs[:10])

        # Top-3 ë°˜í™˜
        doc_ids = []
        for doc in reranked_docs[:3]:
            doc_id = doc.metadata.get('doc_id')
            if doc_id:
                doc_ids.append(doc_id)

        return doc_ids
```

### 2. ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±

```python
class MultiQuerySearchSystem(IRSearchSystem):
    def __init__(self, vectorstore):
        super().__init__(vectorstore)

        self.multi_query_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ 3ê°œì˜ ë‹¤ë¥¸ ê´€ì ì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ê° ì¿¼ë¦¬ëŠ” ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

í˜•ì‹:
1. ì²« ë²ˆì§¸ ì¿¼ë¦¬
2. ë‘ ë²ˆì§¸ ì¿¼ë¦¬
3. ì„¸ ë²ˆì§¸ ì¿¼ë¦¬"""),
            ("user", "{question}")
        ])

    def generate_multiple_queries(self, question: str) -> List[str]:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±"""
        chain = self.multi_query_prompt | self.llm
        response = chain.invoke({"question": question})

        lines = response.content.strip().split('\n')
        queries = []

        for line in lines:
            if line.strip() and any(line.startswith(str(i)) for i in range(1, 4)):
                query = line.split('.', 1)[1].strip()
                queries.append(query)

        return queries

    def multi_query_search(self, query: str, chat_history: str = "") -> List[str]:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰"""

        analysis = self.analyzer.analyze_query(query)
        if not analysis["needs_search"]:
            return []

        # ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±
        queries = self.generate_multiple_queries(query)

        # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        all_docs = []
        for q in queries:
            docs = self.vectorstore.similarity_search(q, k=5)
            all_docs.extend(docs)

        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ ê¸°ë°˜ ì ìˆ˜
        doc_scores = {}
        for doc in all_docs:
            doc_id = doc.metadata.get('doc_id')
            if doc_id:
                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1

        # ë¹ˆë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_docs = sorted(doc_scores.items(),
                             key=lambda x: x[1], reverse=True)

        return [doc_id for doc_id, _ in sorted_docs[:3]]
```

---

## ğŸ¯ ì™„ì „í•œ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸

```python
def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

    print("=== IR ê²½ì§„ëŒ€íšŒ Submission ìƒì„± íŒŒì´í”„ë¼ì¸ ===")

    # 1. ë¬¸ì„œ ë¡œë”©
    print("\n1. ë¬¸ì„œ ë°ì´í„° ë¡œë”©...")
    loader = DocumentLoader("documents.csv")
    documents = loader.load_documents()

    # 2. ë¬¸ì„œ ì²­í‚¹
    print("\n2. ë¬¸ì„œ ì²­í‚¹...")
    chunker = DocumentChunker(chunk_size=500, chunk_overlap=50)
    chunked_documents = chunker.chunk_documents(documents)

    # 3. Vector Store êµ¬ì¶•
    print("\n3. Vector Store êµ¬ì¶•...")
    builder = VectorStoreBuilder()
    vectorstore = builder.build_chroma_store(chunked_documents)

    # 4. ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n4. ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
    search_system = IRSearchSystem(vectorstore, top_k=10)

    # 5. Submission ìƒì„±
    print("\n5. Submission íŒŒì¼ ìƒì„±...")
    generator = SubmissionGenerator(search_system)
    eval_data = generator.load_evaluation_data("evaluation_data.json")
    submission_df = generator.generate_submission(eval_data)

    # 6. ê²€ì¦
    print("\n6. íŒŒì¼ ê²€ì¦...")
    validator = SubmissionValidator()
    is_valid = validator.validate_submission("submission.csv")

    if is_valid:
        validator.analyze_submission("submission.csv")
        print("\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! submission.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ê²€ì¦ ì‹¤íŒ¨. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
```

---

## ğŸ’¡ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ì„ë² ë”© ëª¨ë¸ ì„ íƒ

```python
# ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ ì‹œë„
embedding_options = [
    "text-embedding-3-small",  # ë¹ ë¥´ê³  ì €ë ´
    "text-embedding-3-large",  # ë” ë†’ì€ ì„±ëŠ¥
    "text-embedding-ada-002"  # ê¸°ë³¸ ëª¨ë¸
]
```

### 2. ì²­í‚¹ ì „ëµ ìµœì í™”

```python
# ê³¼í•™ ë¬¸ì„œ íŠ¹ì„±ì— ë§ëŠ” ì²­í‚¹
chunker = DocumentChunker(
    chunk_size=800,  # ê³¼í•™ ê°œë… ì„¤ëª…ì— ì¶©ë¶„í•œ í¬ê¸°
    chunk_overlap=100,  # ì ì ˆí•œ ì˜¤ë²„ë©
    separators=["\n\n", "\n", ". ", " "]  # ê³¼í•™ ë¬¸ì„œ êµ¬ì¡° ê³ ë ¤
)
```

### 3. ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ íŠœë‹

```python
# ë‹¤ì–‘í•œ ê²€ìƒ‰ ì„¤ì • ì‹¤í—˜
search_configs = [
    {"k": 5, "score_threshold": 0.7},
    {"k": 10, "score_threshold": 0.6},
    {"k": 15, "score_threshold": 0.5}
]
```
